{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import os\n",
    "import yaml\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration settings for the YOLO model.\"\"\"\n",
    "    img_size: int\n",
    "    batch_size: int\n",
    "    epochs: int\n",
    "    conf_thres: float\n",
    "    iou_thres: float\n",
    "    pretrained_model: str = \"yolov8s.pt\"\n",
    "    \n",
    "class DetectionSystem:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        classes: List[str],\n",
    "        config: Optional[ModelConfig] = None,\n",
    "        root_dir: Optional[pathlib.Path] = None,\n",
    "    ):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.classes = classes\n",
    "        self.config = config or ModelConfig(\n",
    "            img_size=640,\n",
    "            batch_size=16,\n",
    "            epochs=3,\n",
    "            conf_thres=0.5,\n",
    "            iou_thres=0.45\n",
    "        )\n",
    "        \n",
    "        # Check if classes are provided, if not, raise an error or use a default\n",
    "        if not self.classes:\n",
    "            raise ValueError(\"Classes must be provided during initialization.\")\n",
    "        print(f\"Classes provided during initialization: {self.classes}\")\n",
    "\n",
    "        \n",
    "        # Set up directory structure\n",
    "        self.root_dir = pathlib.Path(root_dir or \"detection_project\")\n",
    "        self.directories = self._initialize_directories()\n",
    "        self._create_data_yaml()\n",
    "        \n",
    "        # Configure device\n",
    "        self.device = self._setup_device()\n",
    "\n",
    "    def train_model(self):\n",
    "        model = YOLO(self.config.pretrained_model)\n",
    "        model.train(\n",
    "            data=self.data_yaml_path,\n",
    "            imgsz=self.config.img_size,\n",
    "            epochs=self.config.epochs,\n",
    "            batch=self.config.batch_size,\n",
    "            conf=self.config.conf_thres,\n",
    "            iou=self.config.iou_thres,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def _setup_device() -> torch.device:\n",
    "    \"\"\"Set up and configure the processing device (GPU/CPU).\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"WARNING: CUDA is not available. Using CPU instead.\")\n",
    "        return torch.device(\"cpu\")\n",
    "            \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.backends.cudnn.benchmark = True  # Optimize for fixed input size\n",
    "    torch.backends.cudnn.deterministic = False # Optimize for speed\n",
    "    torch.cuda.empty_cache()  # Free unused GPU memory\n",
    "        \n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    return device\n",
    "\n",
    "def _initialize_directories(self) -> Dict[str, pathlib.Path]:\n",
    "    \"\"\"Initialize and create project directories.\"\"\"\n",
    "    directories = {\n",
    "        \"models\": self.root_dir / \"models\",\n",
    "        \"data\": self.root_dir / \"data\",\n",
    "        \"raw_data\": self.root_dir / \"data\" / \"raw_data\",\n",
    "        \"processed_data\": self.root_dir / \"data\" / \"processed_data\",\n",
    "        \"yolo_data\": self.root_dir / \"data\" / \"yolo_format\",\n",
    "        \"train\": self.root_dir / \"data\" / \"yolo_format\" / \"train\",\n",
    "        \"val\": self.root_dir / \"data\" / \"yolo_format\" / \"val\",\n",
    "        \"test\": self.root_dir / \"data\" / \"yolo_format\" / \"test\",\n",
    "        \"results\": self.root_dir / \"results\",\n",
    "        \"logs\": self.root_dir / \"logs\"\n",
    "    }\n",
    "\n",
    "    for dir_path in directories.values():\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    self._clear_yolo_directories(directories)\n",
    "    return directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def _clear_yolo_directories(directories: Dict[str, pathlib.Path]) -> None:\n",
    "    \"\"\"Clear YOLO data directories and create necessary subdirectories.\"\"\"\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        split_dir = directories[split]\n",
    "        for subdir in [\"images\", \"labels\"]:\n",
    "            subdir_path = split_dir / subdir\n",
    "            if subdir_path.exists():\n",
    "                shutil.rmtree(subdir_path)  # Ensure previous data is removed\n",
    "            subdir_path.mkdir(parents=True)\n",
    "\n",
    "        label_cache = split_dir / \"labels.cache\"\n",
    "        if label_cache.exists():\n",
    "            os.remove(str(label_cache))\n",
    "    \n",
    "\n",
    "def _create_data_yaml(self) -> None:\n",
    "    \"\"\"Create YAML configuration for YOLO training.\"\"\"\n",
    "    yaml_content = {\n",
    "        \"train\": str(self.directories[\"train\"].resolve() / \"images\"),\n",
    "        \"val\": str(self.directories[\"val\"].resolve() / \"images\"),\n",
    "        \"test\": str(self.directories[\"test\"].resolve() / \"images\"),\n",
    "        \"nc\": len(self.classes),\n",
    "        \"names\": self.classes\n",
    "    }\n",
    "\n",
    "    yaml_path = self.directories[\"data\"] / \"data.yaml\"\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.safe_dump(yaml_content, f)\n",
    "\n",
    "    self.data_yaml_path = yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def _dataset_context(self, dataset_name: str):\n",
    "    \"\"\"Context manager for dataset handling with proper cleanup.\"\"\"\n",
    "    try:\n",
    "        yield\n",
    "    except Exception as e:\n",
    "        if fo.dataset_exists(dataset_name):\n",
    "            fo.delete_dataset(dataset_name)\n",
    "        raise e\n",
    "\n",
    "def _create_splits(\n",
    "    self, \n",
    "    dataset: fo.Dataset, \n",
    "    split_ratios: List[float] = [0.8, 0.1, 0.1]\n",
    ") -> Tuple[fo.Dataset, fo.Dataset, fo.Dataset]:\n",
    "    \"\"\"\n",
    "    Create train, validation, and test splits from the dataset.\n",
    "        \n",
    "    Args:\n",
    "        dataset: The dataset to split\n",
    "        split_ratios: List of ratios for [train, val, test] splits\n",
    "            \n",
    "    Returns:\n",
    "        Tuple of (train_split, val_split, test_split)\n",
    "    \"\"\"\n",
    "    if abs(sum(split_ratios) - 1.0) > 1e-5:  # Robust check for ratios\n",
    "        raise ValueError(\"Split ratios must sum to 1\")\n",
    "\n",
    "    # Ensure dataset view for balanced class distribution, \n",
    "    dataset.shuffle(seed=51)\n",
    "\n",
    "    train_size = int(split_ratios[0] * len(dataset))\n",
    "    val_size = int(split_ratios[1] * len(dataset))\n",
    "\n",
    "    train_view = dataset.take(train_size)\n",
    "    val_view = dataset.skip(train_size).take(val_size)\n",
    "    test_view = dataset.skip(train_size + val_size)\n",
    "\n",
    "    train_split = train_view.clone()\n",
    "\n",
    "    val_split = val_view.clone()\n",
    "    test_split = test_view.clone()\n",
    "\n",
    "\n",
    "\n",
    "    # Add tags to the splits\n",
    "    train_split.tag_samples(\"train\")\n",
    "    val_split.tag_samples(\"val\")\n",
    "    test_split.tag_samples(\"test\")\n",
    "\n",
    "    return train_split, val_split, test_split\n",
    "\n",
    "def _export_splits(\n",
    "    self, \n",
    "    dataset: fo.Dataset, \n",
    "    splits: List[fo.Dataset]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Export the splits to YOLO format.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The original dataset\n",
    "        splits: List of [train_split, val_split, test_split]\n",
    "    \"\"\"\n",
    "    split_names = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    for split, split_name in zip(splits, split_names):\n",
    "        dir_path = self.directories[split_name]\n",
    "        print(f\"Exporting {split_name} split to {dir_path}\")\n",
    "\n",
    "        split.export(\n",
    "            export_dir=str(dir_path / \"images\"),\n",
    "            dataset_type=fo.types.YOLOv5Dataset,\n",
    "            label_field=\"ground_truth\",\n",
    "            classes=self.classes,  # Provide explicit classes to be exported\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "def prepare_dataset(\n",
    "        self,\n",
    "        confidence_thresh: float = 0.04,\n",
    "        area_thresh: float = 0.01,\n",
    "        samples_per_class: int = 600\n",
    "\n",
    "    ) -> Tuple[fo.Dataset, Dict[str, int]]:\n",
    "        \"\"\"Prepare and process the dataset with balanced class distribution.\"\"\"\n",
    "        dataset_name = f\"{self.dataset_name}--{int(time.time())}\"\n",
    "        self.dataset_name = dataset_name  # store the dataset name as a class property for later access\n",
    "        with self._dataset_context(dataset_name):  # Handle cleanup upon exception\n",
    "            merged_dataset = fo.Dataset(dataset_name)  # Persistent to avoid duplicates\n",
    "            merged_dataset.persistent = True\n",
    "\n",
    "            # Process each class with sample limit\n",
    "\n",
    "            for cls in self.classes:\n",
    "                class_view = foz.load_zoo_dataset(\n",
    "                    \"open-images-v7\",\n",
    "                    split=\"validation\",\n",
    "                    label_types=[\"detections\"],\n",
    "                    classes=[cls],  # Filter by class\n",
    "                    seed=51,  # Seed to guarantee same sampling\n",
    "                    shuffle=True,\n",
    "                    max_samples=samples_per_class\n",
    "                ).clone()\n",
    "                    \n",
    "                merged_dataset.merge_samples(class_view)\n",
    "                \n",
    "            # # Add a dummy 'ground_truth' detections field to each sample\n",
    "            # for sample in merged_dataset:\n",
    "            #     sample[\"ground_truth\"] = fo.Detections(detections=[])\n",
    "            #     sample.save()\n",
    "\n",
    "            train_split, val_split, test_split = self._create_splits(merged_dataset)\n",
    "\n",
    "            self._export_splits(merged_dataset, [train_split, val_split, test_split])\n",
    "                \n",
    "            class_distribution = merged_dataset.count_values(\"ground_truth.detections.label\")\n",
    "\n",
    "            print(\"Class Distribution:\", json.dumps(class_distribution, indent=2))\n",
    "\n",
    "            return merged_dataset, class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(self, model: YOLO) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model's performance.\n",
    "            \n",
    "    Args:\n",
    "        model: Trained YOLO model\n",
    "            \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "\n",
    "    results = model.val(data=self.data_yaml_path, device=self.device)\n",
    "        \n",
    "    test_dataset = fo.load_dataset(self.dataset_name).match_tags(\"test\")  # Load existing dataset for evaluation\n",
    "    fps = self._calculate_fps(model, test_dataset)  # Evaluate on test samples to estimate performance\n",
    "\n",
    "\n",
    "    results_dict = results.results_dict\n",
    "\n",
    "\n",
    "    final_results = {\n",
    "\n",
    "\n",
    "        \"mAP50-95\": results_dict.get('metrics/mAP50-95(B)'),\n",
    "        \"mAP50\": results_dict.get(\"metrics/mAP50(B)\"),    \n",
    "        \"precision\": results_dict.get(\"metrics/precision(B)\"),  \n",
    "        \"recall\": results_dict.get(\"metrics/recall(B)\"),  \n",
    "        \"fps\": fps,\n",
    "\n",
    "\n",
    "        \"inference_device\": str(self.device),  \n",
    "        \"class_metrics\": {\n",
    "            cls: {\n",
    "                \"precision\": results_dict.get(f\"metrics/{cls}_precision\"),  \n",
    "                \"recall\": results_dict.get(f\"metrics/{cls}_recall\"),  \n",
    "                \"mAP50\": results_dict.get(f\"metrics/{cls}_mAP50\")  \n",
    "            }\n",
    "            for cls in self.classes\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    results_path = self.directories[\"results\"] / \"evaluation_results.json\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "\n",
    "        json.dump(final_results, f, indent=4)  # Ensure indentation\n",
    "\n",
    "    return final_results\n",
    "\n",
    "def _calculate_fps(self, model: YOLO, dataset: fo.Dataset) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate frames per second on test dataset.\n",
    "        \n",
    "    Args:\n",
    "        model: Trained YOLO model\n",
    "        dataset: Test dataset\n",
    "            \n",
    "    Returns:\n",
    "        Average FPS\n",
    "    \"\"\"\n",
    "    print(\"Calculating FPS...\")\n",
    "    times = []\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "\n",
    "\n",
    "        for sample in tqdm(dataset, desc=\"FPS Test\"):\n",
    "\n",
    "            img = cv2.imread(sample.filepath)\n",
    "            if img is None:\n",
    "\n",
    "                raise ValueError(\"Image cannot be loaded\") # Or appropriate error handling\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "            _ = model.predict(source=[img], device=self.device)\n",
    "\n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "    return 1.0 / np.mean(times) if times else 0.0  # Proper averaging logic for timings\n",
    "\n",
    "def _filter_detections(\n",
    "    self,\n",
    "    dataset: fo.Dataset,\n",
    "    cls: str,\n",
    "    confidence_thresh: float,\n",
    "    area_thresh: float\n",
    ") -> None:\n",
    "    \"\"\"Filter detections based on confidence and area thresholds.\"\"\"\n",
    "\n",
    "    # Check if \"ground_truth\" field exists\n",
    "    if \"ground_truth\" not in dataset.get_field_schema():\n",
    "        print(f\"Warning: 'ground_truth' field not found in dataset. Skipping filtering for class '{cls}'.\")\n",
    "        return\n",
    "\n",
    "    # Check if \"detections\" field exists within \"ground_truth\"\n",
    "    if \"detections\" not in dataset.get_field_schema(field=\"ground_truth\"):\n",
    "        print(f\"Warning: 'detections' field not found within 'ground_truth' for class '{cls}'. Skipping filtering.\")\n",
    "        return\n",
    "\n",
    "    # If both fields exist, proceed with filtering\n",
    "    dataset.filter_labels(\n",
    "        \"ground_truth\",\n",
    "        (F(\"confidence\") > confidence_thresh) &\n",
    "        (F(\"bounding_box\")[2] * F(\"bounding_box\")[3] >= area_thresh),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def _convert_classifications_to_detections(dataset: fo.Dataset) -> None:\n",
    "    \"\"\"Convert classification labels to detection format.\"\"\"\n",
    "        \n",
    "class FPSCounter:\n",
    "    \"\"\"Utility class for FPS calculation.\"\"\"\n",
    "    def __init__(self, avg_frames: int = 30):\n",
    "        self.avg_frames = avg_frames\n",
    "        self.times = []\n",
    "        self.fps = 0\n",
    "\n",
    "    def update(self) -> None:\n",
    "        \"\"\"Update internal timers and recalculate FPS.\"\"\"\n",
    "        self.times.append(time.time())\n",
    "\n",
    "\n",
    "\n",
    "        if len(self.times) > self.avg_frames:\n",
    "            self.times.pop(0)\n",
    "\n",
    "        # if self.times:\n",
    "        #     self.fps = len(self.times) / (self.times[-1] - self.times[0])  # Correct timeframe if self.times is not empty\n",
    "\n",
    "class StreamProcessor:\n",
    "    \"\"\"Handle real-time video processing with optimized GPU usage.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: YOLO,\n",
    "        device: torch.device,\n",
    "        img_size: int,\n",
    "        conf_thres: float,\n",
    "        iou_thres: float\n",
    "\n",
    "    ):\n",
    "        self.model = model.to(device)  # Send to device immediately on init\n",
    "        self.device = device  # Ensure device configuration for GPU use\n",
    "        self.img_size = img_size\n",
    "        self.conf_thres = conf_thres\n",
    "        self.iou_thres = iou_thres\n",
    "        self.fps_counter = FPSCounter()\n",
    "\n",
    "    \n",
    "\n",
    "    def process_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Process a single frame with GPU acceleration.\"\"\"\n",
    "        img = self._preprocess_frame(frame)\n",
    "        with torch.inference_mode():\n",
    "            results = self.model(img, conf=self.conf_thres, iou=self.iou_thres)[0]\n",
    "        self.fps_counter.update()\n",
    "        results = results.plot()\n",
    "        return cv2.cvtColor(results, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    def _preprocess_frame(self, frame: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Preprocess frame for model input.\"\"\"\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        img = torch.from_numpy(img).to(self.device).float().div(255)\n",
    "        img = img.permute(2, 0, 1)\n",
    "        img = img.unsqueeze(0)\n",
    "        return img\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes provided during initialization: ['Cat', 'Dog', 'Bird', 'Man', 'Woman', 'Human face']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DetectionSystem' object has no attribute '_initialize_directories'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 54\u001b[0m\n\u001b[0;32m     49\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Main execution function with improved error handling.\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m ModelConfig(\n\u001b[0;32m      5\u001b[0m     img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m,\n\u001b[0;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     iou_thres\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.45\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m system \u001b[38;5;241m=\u001b[39m \u001b[43mDetectionSystem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopen-images-v7-dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBird\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWoman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHuman face\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m dataset, class_dist \u001b[38;5;241m=\u001b[39m system\u001b[38;5;241m.\u001b[39mprepare_dataset()\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m system\u001b[38;5;241m.\u001b[39mtrain_model()\n",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m, in \u001b[0;36mDetectionSystem.__init__\u001b[1;34m(self, dataset_name, classes, config, root_dir)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Set up directory structure\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(root_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetection_project\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_directories\u001b[49m()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_data_yaml()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Configure device\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DetectionSystem' object has no attribute '_initialize_directories'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    \"\"\"Main execution function with improved error handling.\"\"\"\n",
    "    config = ModelConfig(\n",
    "        img_size=640,\n",
    "        batch_size=16,\n",
    "        epochs=3,\n",
    "        conf_thres=0.5,\n",
    "        iou_thres=0.45\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    system = DetectionSystem(\n",
    "        dataset_name=\"open-images-v7-dataset\",\n",
    "        classes=[\"Cat\", \"Dog\", \"Bird\", \"Man\", \"Woman\",\"Human face\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    dataset, class_dist = system.prepare_dataset()\n",
    "    model = system.train_model()\n",
    "    results = system.evaluate_model(model)\n",
    "\n",
    "\n",
    "\n",
    "    processor = StreamProcessor(\n",
    "        model=model,\n",
    "        device=system.device,\n",
    "        img_size=config.img_size,\n",
    "        conf_thres=config.conf_thres,\n",
    "        iou_thres=config.iou_thres\n",
    "    )\n",
    "\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        processed_frame = processor.process_frame(frame)\n",
    "\n",
    "        cv2.imshow(\"Detection\", processed_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
